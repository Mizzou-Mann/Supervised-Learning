% assignment_2.tex
% CS 8725 - Supervised Learning (Fall 2015)
%     University of Missouri-Columbia
%             Chanmann Lim
%            September 2015

\documentclass[a4paper]{article}

\usepackage[margin=1 in]{geometry}
\usepackage{amsmath}
\usepackage{float}

\everymath{\displaystyle}
\DeclareMathOperator*{\argmax}{\arg\!\max}

\begin{document}
\title{CS 8725: Report for assignment 2}
\author{Chanmann Lim}
\date{September 9, 2015}
\maketitle

\paragraph{1.} For $Y \in \{T, F\}$, one parameter is needed to describe $P(Y)$, two parameters are needed to describe $P(X_1|Y)$ and four parameters $(\mu_{iY}, \sigma_{iY})$ are needed to describe $P(X_i|Y)$ for $2 \leq i \leq d$.\\

	$ P(Y) $
	
	$ P(X_1|Y), \qquad Y \in \{T, F\}, \; X_1 \in \{T, F\} $
	
	$ P(X_i|Y) \sim N(\mu_{iY}, \sigma_{iY}^2), \qquad Y \in \{T, F\}, \; 2 \leq i \leq d $
	
	The total number of parameters = $1 + 2 + 4 \times (d-1) = 4d - 1 $.	
	\begin{align}
		P(Y|X) &= \frac{P(X|Y) \cdot P(Y)}{P(X)} \\
			&\propto P(X|Y) \cdot P(Y) \\
			&= P(X_1|Y) \cdot \prod_{i=2}^d N(x_i; \mu_{iY}, \sigma_{iY}^2) \cdot P(Y)
	\end{align}
	Where, $N(x_i; \mu_{iY}, \sigma_{iY}^2) = \frac{1}{\sqrt{2\pi}\sigma_{iY}}e^{-\frac{(x_i-\mu_{iY})^2}{2\sigma_{iY}}}$

\paragraph{2. (a)} Naive Bayes decision rule is the function that maximizes the posterior probability however the denominator of the posterior probability is merely the normalization term to satisfy the property of probability and it does not depend on $y$ thus it can be dropped.\\
	\begin{align}
		f_{NB}(Sunny, Windy) = \argmax_{y} P(Sunny|y) \cdot P(Windy|y) \cdot P(y)
	\end{align}
	Where $Y \in \{Hike, \neg Hike\}$ and 
	$$ P(Y = Hike) = P(Y = \neg Hike) = 0.5 $$
	$P(y)$ can also be dropped.
	\begin{align}
		f_{NB}(Sunny, Windy) = \argmax_{y} P(Sunny|y) \cdot P(Windy|y)
	\end{align}
	Therefore, predict Alice and Bob go hiking if $P(Sunny|Hike) \times P(Windy|Hike) > P(Sunny|\neg Hike) \times P(Windy|\neg Hike)$ and else otherwise.
	
\paragraph{(b)} The joint probability of Alice and Bob go hiking and the weather is sunny and windy can be considered as the product of the likelihood of the weather given they go hiking $P(Sunny, Windy | Hike)$ and the prior probability of going hiking $P(Hike)$.\\
	\begin{align}
		P(Sunny, Windy, Hike) &= P(Sunny, Windy | Hike) \cdot P(Hike)\\
			&= P(Sunny | Hike) \cdot P(Windy | Hike) \cdot P(Hike)\\
			&= 0.8 \times 0.4 \times 0.5\\
			&= 0.16
	\end{align}
	Similarly,
	\begin{align}
		P(Sunny, Windy, \neg Hike) &= P(Sunny | \neg Hike) \cdot P(Windy | \neg Hike) \cdot P(\neg Hike)\\
			&= 0.7 \times 0.5 \times 0.5\\
			&= 0.175
	\end{align}
	
	The prediction of Alice and Bob go hiking when the weather is sunny and windy $P(Hike|Sunny, Windy)$ is equal to the probability that they actually go hiking and it is sunny and windy $P(Sunny, Windy, Hike)$ divided by the probability that it's sunny and windy regardless of hiking $P(Sunny, Windy)$.
	$$ P(Hike|Sunny, Windy) = \frac{P(Sunny, Windy, Hike)}{P(Sunny, Windy)} $$
	
	Therefore the probability that the weather is sunny and windy and Alice and Bob isn't go hiking $P(Sunny, Windy, \neg Hike)$ is in fact the probability of prediction error of $P(Hike|Sunny, Windy)$. 
	\begin{align}
		P_e &= P(Sunny, Windy,\neg Hike) \\
			&= 0.175
	\end{align}
	
\end{document}